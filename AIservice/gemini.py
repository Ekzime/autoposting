###############################
#           system libs
#------------------------------
import re
import json
import hashlib
from typing import List, Optional, Dict, Any, Union
###############################
#           my moduls
#------------------------------
from .prompts import prompt
from config import settings
###############################
#            FAST API
#------------------------------
from fastapi import FastAPI, APIRouter
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
###############################
#       other libs/frameworks
#------------------------------
import google.generativeai as genai

# Configure Gemini API
genai.configure(api_key=settings.ai_service.gemini_key)
model = genai.GenerativeModel("gemini-1.5-flash")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–µ—à –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤
processed_content_hashes = set()

# –ú–æ–¥–µ–ª—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
class PostBatch(BaseModel):
    posts: List[str]
    has_image: Optional[bool] = False

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI()
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS –¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∫—Ä–æ—Å—Å-–¥–æ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
app.add_middleware(
    CORSMiddleware,
    allow_origins=    ["*"],  # –†–∞–∑—Ä–µ—à–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å –ª—é–±—ã—Ö –¥–æ–º–µ–Ω–æ–≤
    allow_methods=    ["*"],  # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ HTTP –º–µ—Ç–æ–¥—ã
    allow_headers=    ["*"],  # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    allow_credentials=True,   # –†–∞–∑—Ä–µ—à–∞–µ–º –ø–µ—Ä–µ–¥–∞—á—É —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
)


def generate_content_hash(text: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö–µ—à –¥–ª—è —Ç–µ–∫—Å—Ç–∞, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ä–µ–≥–∏—Å—Ç—Ä"""
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    normalized = re.sub(r'[^\w\s]', '', text.lower())
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    normalized = ' '.join(normalized.split())
    return hashlib.md5(normalized.encode()).hexdigest()


def check_content_similarity(new_text: str, existing_hashes: set) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ—Ö–æ–∂ –ª–∏ –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ"""
    new_hash = generate_content_hash(new_text)
    return new_hash in existing_hashes


def filter_duplicate_results(results: List[Dict]) -> List[Dict]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ AI"""
    if not results:
        return results
    
    filtered_results = []
    session_hashes = set()
    
    for result in results:
        text = result.get('text', '')
        if not text:
            continue
            
        content_hash = generate_content_hash(text)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫ –Ω–∞ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ, —Ç–∞–∫ –∏ –Ω–∞ —Å–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
        if content_hash not in processed_content_hashes and content_hash not in session_hashes:
            filtered_results.append(result)
            session_hashes.add(content_hash)
            processed_content_hashes.add(content_hash)
            print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç: {text[:50]}...")
        else:
            print(f"‚ùå –î—É–±–ª–∏–∫–∞—Ç –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω: {text[:50]}...")
    
    return filtered_results


# –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ Gemini API
def process_posts(posts: list[str], has_image: bool = False, prompt_template: str = prompt) -> list[str]:
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–∞ –≤—Ö–æ–¥–µ
    unique_posts = []
    for post in posts:
        if not check_content_similarity(post, processed_content_hashes):
            unique_posts.append(post)
        else:
            print(f"üîÑ –í—Ö–æ–¥–Ω–æ–π –ø–æ—Å—Ç —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Ä–∞–Ω–µ–µ: {post[:50]}...")
    
    if not unique_posts:
        print("üö´ –í—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –ø–æ—Å—Ç—ã —è–≤–ª—è—é—Ç—Å—è –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏")
        return []
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç, –¥–æ–±–∞–≤–ª—è—è –ø–æ—Å—Ç—ã –≤ –≤–∏–¥–µ —Å–ø–∏—Å–∫–∞
    content = prompt_template + "\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–ª–∏—á–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    if has_image:
        content += "–í–ê–ñ–ù–û: –ö —Å–æ–æ–±—â–µ–Ω–∏—é –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ –ø–æ—Å—Ç.\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ—Å—Ç–æ–≤
    if len(unique_posts) > 1:
        content += f"–í–ù–ò–ú–ê–ù–ò–ï: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è {len(unique_posts)} –ø–æ—Å—Ç–æ–≤. –£–±–µ–¥–∏—Å—å, —á—Ç–æ –∫–∞–∂–¥—ã–π –≤—ã—Ö–æ–¥–Ω–æ–π –ø–æ—Å—Ç —É–Ω–∏–∫–∞–ª–µ–Ω –∏ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç —Å–º—ã—Å–ª –¥—Ä—É–≥–∏—Ö.\n\n"
    
    content += "\n".join(f"- {p}" for p in unique_posts)

    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Gemini API
        response = model.generate_content(content)
        raw = response.text.strip()

        # –í—ã–≤–æ–¥–∏–º —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print("üì• GEMINI RAW RESPONSE:")
        print(raw)

        # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
        parsed_results = []
        
        # 1. –ï—Å–ª–∏ markdown-–æ–±—ë—Ä—Ç–∫–∞ ```json ... ```, –≤—ã—Ä–µ–∑–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        match = re.search(r"```(?:json)?\s*(\[.*?\])\s*```", raw, re.DOTALL)
        if match:
            parsed_results = json.loads(match.group(1))
        # 2. –ï—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ –º–∞—Å—Å–∏–≤ –±–µ–∑ markdown
        elif raw.startswith("[") and raw.endswith("]"):
            parsed_results = json.loads(raw)
        # 3. –ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî —Ä–∞—Å–ø–∞—Ä—Å–∏–º –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏
        else:
            lines = raw.splitlines()
            parsed_results = [{"text": line.strip("-‚Ä¢ ").strip()} for line in lines if line and not line.startswith("–í–æ—Ç")]

        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        filtered_results = filter_duplicate_results(parsed_results)
        
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(parsed_results)} -> {len(filtered_results)} (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)")
        
        return filtered_results

    except Exception as e:
        # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
        print(f"‚ùå Gemini Error: {e}")
        return []


# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ—Å—Ç–æ–≤
@app.post('/gemini/filter')
async def multi_filter(data: PostBatch):
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Å—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    result = process_posts(posts=data.posts, has_image=data.has_image)
    return {
        'status': 'success',
        'result': result,
    }


# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∫–µ—à–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (–¥–ª—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Ü–µ–ª–µ–π)
@app.post('/gemini/clear_cache')
async def clear_duplicate_cache():
    """–û—á–∏—â–∞–µ—Ç –∫–µ—à –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    global processed_content_hashes
    cache_size = len(processed_content_hashes)
    processed_content_hashes.clear()
    return {
        'status': 'success',
        'message': f'–ö–µ—à –æ—á–∏—â–µ–Ω. –£–¥–∞–ª–µ–Ω–æ {cache_size} –∑–∞–ø–∏—Å–µ–π.'
    }


# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–µ—à–∞
@app.get('/gemini/cache_stats')
async def get_cache_stats():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–µ—à–∞"""
    return {
        'status': 'success',
        'cache_size': len(processed_content_hashes),
        'recent_hashes': list(processed_content_hashes)[-10:] if processed_content_hashes else []
    }
